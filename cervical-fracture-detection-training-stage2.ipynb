{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training 2.5D Classification Stage2\n\nThis is small version of training the final models (stage2), using efficientnetv2_s as backbone, and 224x224 as input.\n\nAfter all stage1 models are trained, then we can use those models to predict 3D masks for all training samples (2k)\n\nThen use those predicted masks to crop out all vertebraes (2k * 7 = 14k)\n\nNow let's use this dataset to train a 2.5D classification with LSTM ","metadata":{}},{"cell_type":"code","source":"!pip -q install timm","metadata":{"execution":{"iopub.status.busy":"2022-12-30T15:45:53.668100Z","iopub.execute_input":"2022-12-30T15:45:53.669135Z","iopub.status.idle":"2022-12-30T15:46:07.217315Z","shell.execute_reply.started":"2022-12-30T15:45:53.669035Z","shell.execute_reply":"2022-12-30T15:46:07.216127Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"DEBUG = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-30T15:46:07.220212Z","iopub.execute_input":"2022-12-30T15:46:07.220616Z","iopub.status.idle":"2022-12-30T15:46:07.225945Z","shell.execute_reply.started":"2022-12-30T15:46:07.220570Z","shell.execute_reply":"2022-12-30T15:46:07.224525Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport ast\nimport cv2\nimport time\nimport timm\nimport pickle\nimport random\nimport argparse\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom PIL import Image\nfrom tqdm import tqdm\nimport albumentations\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.cuda.amp as amp\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n%matplotlib inline\nrcParams['figure.figsize'] = 20, 8\ndevice = torch.device('cuda')\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2022-12-30T15:46:07.227666Z","iopub.execute_input":"2022-12-30T15:46:07.228004Z","iopub.status.idle":"2022-12-30T15:46:11.431203Z","shell.execute_reply.started":"2022-12-30T15:46:07.227972Z","shell.execute_reply":"2022-12-30T15:46:11.429938Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"kernel_type = '0920_1bonev2_effv2s_224_15_6ch_augv2_mixupp5_drl3_rov1p2_bs8_lr23e5_eta23e6_50ep'\nload_kernel = None\nload_last = True\n\nn_folds = 5\nbackbone = 'tf_efficientnetv2_s_in21ft1k'\n\nimage_size = 224\nn_slice_per_c = 15\nin_chans = 6\n\ninit_lr = 23e-5\neta_min = 23e-6\nbatch_size = 8\ndrop_rate = 0.\ndrop_rate_last = 0.3\ndrop_path_rate = 0.\np_mixup = 0.5\np_rand_order_v1 = 0.2\n\ndata_dir = '../input/rsna-cropped-2d-224-0920-2m/cropped_2d_224_15_ext0_5ch_0920_2m/cropped_2d_224_15_ext0_5ch_0920_2m'\nuse_amp = True\nnum_workers = 4\nout_dim = 1\n\nn_epochs = 75\n\nlog_dir = './logs'\nmodel_dir = './models'\nos.makedirs(log_dir, exist_ok=True)\nos.makedirs(model_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T15:46:11.433731Z","iopub.execute_input":"2022-12-30T15:46:11.434684Z","iopub.status.idle":"2022-12-30T15:46:11.443671Z","shell.execute_reply.started":"2022-12-30T15:46:11.434643Z","shell.execute_reply":"2022-12-30T15:46:11.442697Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"transforms_train = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.VerticalFlip(p=0.5),\n    albumentations.Transpose(p=0.5),\n    albumentations.RandomBrightness(limit=0.1, p=0.7),\n    albumentations.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.3, rotate_limit=45, border_mode=4, p=0.7),\n\n    albumentations.OneOf([\n        albumentations.MotionBlur(blur_limit=3),\n        albumentations.MedianBlur(blur_limit=3),\n        albumentations.GaussianBlur(blur_limit=3),\n        albumentations.GaussNoise(var_limit=(3.0, 9.0)),\n    ], p=0.5),\n    albumentations.OneOf([\n        albumentations.OpticalDistortion(distort_limit=1.),\n        albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n    ], p=0.5),\n\n    albumentations.Cutout(max_h_size=int(image_size * 0.5), max_w_size=int(image_size * 0.5), num_holes=1, p=0.5),\n])\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T15:46:11.445899Z","iopub.execute_input":"2022-12-30T15:46:11.446612Z","iopub.status.idle":"2022-12-30T15:46:11.467613Z","shell.execute_reply.started":"2022-12-30T15:46:11.446575Z","shell.execute_reply":"2022-12-30T15:46:11.466503Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1151: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/blur/transforms.py:185: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/dropout/cutout.py:51: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# DataFrame","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(f'../input/rsna-cropped-2d-224-0920-2m/train_seg.csv'))\ndf = df.sample(16).reset_index(drop=True) if DEBUG else df\n\n\nsid = []\ncs = []\nlabel = []\nfold = []\nfor _, row in df.iterrows():\n    for i in [1,2,3,4,5,6,7]:\n        sid.append(row.StudyInstanceUID)\n        cs.append(i)\n        label.append(row[f'C{i}'])\n        fold.append(row.fold)\n\ndf = pd.DataFrame({\n    'StudyInstanceUID': sid,\n    'c': cs,\n    'label': label,\n    'fold': fold\n})\n\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T15:46:11.471173Z","iopub.execute_input":"2022-12-30T15:46:11.472695Z","iopub.status.idle":"2022-12-30T15:46:11.875418Z","shell.execute_reply.started":"2022-12-30T15:46:11.472659Z","shell.execute_reply":"2022-12-30T15:46:11.874505Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                StudyInstanceUID  c  label  fold\n14121  1.2.826.0.1.3680043.18786  3      0     4\n14122  1.2.826.0.1.3680043.18786  4      0     4\n14123  1.2.826.0.1.3680043.18786  5      0     4\n14124  1.2.826.0.1.3680043.18786  6      0     4\n14125  1.2.826.0.1.3680043.18786  7      1     4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>c</th>\n      <th>label</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14121</th>\n      <td>1.2.826.0.1.3680043.18786</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14122</th>\n      <td>1.2.826.0.1.3680043.18786</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14123</th>\n      <td>1.2.826.0.1.3680043.18786</td>\n      <td>5</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14124</th>\n      <td>1.2.826.0.1.3680043.18786</td>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14125</th>\n      <td>1.2.826.0.1.3680043.18786</td>\n      <td>7</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CLSDataset(Dataset):\n    def __init__(self, df, mode, transform):\n\n        self.df = df.reset_index()\n        self.mode = mode\n        self.transform = transform\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        cid = row.c\n        \n        images = []\n        \n        for ind in list(range(n_slice_per_c)):\n            filepath = os.path.join(data_dir, f'{row.StudyInstanceUID}_{cid}_{ind}.npy')\n            image = np.load(filepath)\n            image = self.transform(image=image)['image']\n            image = image.transpose(2, 0, 1).astype(np.float32) / 255.\n            images.append(image)\n        images = np.stack(images, 0)\n\n        if self.mode != 'test':\n            images = torch.tensor(images).float()\n            labels = torch.tensor([row.label] * n_slice_per_c).float()\n            \n            if self.mode == 'train' and random.random() < p_rand_order_v1:\n                indices = torch.randperm(images.size(0))\n                images = images[indices]\n\n            return images, labels\n        else:\n            return torch.tensor(images).float()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:20.432038Z","iopub.execute_input":"2022-10-29T08:46:20.432358Z","iopub.status.idle":"2022-10-29T08:46:20.444807Z","shell.execute_reply.started":"2022-10-29T08:46:20.432329Z","shell.execute_reply":"2022-10-29T08:46:20.443542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 20,8\n\ndf_show = df\ndataset_show = CLSDataset(df_show, 'train', transform=transforms_train)\nloader_show = torch.utils.data.DataLoader(dataset_show, batch_size=batch_size, shuffle=True, num_workers=num_workers)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:20.446738Z","iopub.execute_input":"2022-10-29T08:46:20.447346Z","iopub.status.idle":"2022-10-29T08:46:20.460511Z","shell.execute_reply.started":"2022-10-29T08:46:20.447288Z","shell.execute_reply":"2022-10-29T08:46:20.45937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axarr = plt.subplots(2,4)\nfor p in range(4):\n    idx = p * 20\n    imgs, lbl = dataset_show[idx]\n    axarr[0, p].imshow(imgs[7][:3].permute(1, 2, 0))\n    axarr[1, p].imshow(imgs[7][-1])","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:20.462202Z","iopub.execute_input":"2022-10-29T08:46:20.462931Z","iopub.status.idle":"2022-10-29T08:46:23.244073Z","shell.execute_reply.started":"2022-10-29T08:46:20.462883Z","shell.execute_reply":"2022-10-29T08:46:23.242914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class TimmModel(nn.Module):\n    def __init__(self, backbone, pretrained=False):\n        super(TimmModel, self).__init__()\n\n        self.encoder = timm.create_model(\n            backbone,\n            in_chans=in_chans,\n            num_classes=out_dim,\n            features_only=False,\n            drop_rate=drop_rate,\n            drop_path_rate=drop_path_rate,\n            pretrained=pretrained\n        )\n\n        if 'efficient' in backbone:\n            hdim = self.encoder.conv_head.out_channels\n            self.encoder.classifier = nn.Identity()\n        elif 'convnext' in backbone:\n            hdim = self.encoder.head.fc.in_features\n            self.encoder.head.fc = nn.Identity()\n\n\n        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n        self.head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(drop_rate_last),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, out_dim),\n        )\n\n    def forward(self, x):  # (bs, nslice, ch, sz, sz)\n        bs = x.shape[0]\n        x = x.view(bs * n_slice_per_c, in_chans, image_size, image_size)\n        feat = self.encoder(x)\n        feat = feat.view(bs, n_slice_per_c, -1)\n        feat, _ = self.lstm(feat)\n        feat = feat.contiguous().view(bs * n_slice_per_c, -1)\n        feat = self.head(feat)\n        feat = feat.view(bs, n_slice_per_c).contiguous()\n\n        return feat\n","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:23.245146Z","iopub.execute_input":"2022-10-29T08:46:23.245545Z","iopub.status.idle":"2022-10-29T08:46:23.26229Z","shell.execute_reply.started":"2022-10-29T08:46:23.245506Z","shell.execute_reply":"2022-10-29T08:46:23.260005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = TimmModel(backbone)\nm(torch.rand(2, n_slice_per_c, in_chans, image_size, image_size)).shape","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:23.26406Z","iopub.execute_input":"2022-10-29T08:46:23.265243Z","iopub.status.idle":"2022-10-29T08:46:32.738976Z","shell.execute_reply.started":"2022-10-29T08:46:23.265207Z","shell.execute_reply":"2022-10-29T08:46:32.73783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss & Metric","metadata":{}},{"cell_type":"code","source":"bce = nn.BCEWithLogitsLoss(reduction='none')\n\n\ndef criterion(logits, targets, activated=False):\n    if activated:\n        losses = nn.BCELoss(reduction='none')(logits.view(-1), targets.view(-1))\n    else:\n        losses = bce(logits.view(-1), targets.view(-1))\n    losses[targets.view(-1) > 0] *= 2.\n    norm = torch.ones(logits.view(-1).shape[0]).to(device)\n    norm[targets.view(-1) > 0] *= 2\n    return losses.sum() / norm.sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:32.740646Z","iopub.execute_input":"2022-10-29T08:46:32.741112Z","iopub.status.idle":"2022-10-29T08:46:32.74936Z","shell.execute_reply.started":"2022-10-29T08:46:32.741073Z","shell.execute_reply":"2022-10-29T08:46:32.747958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Valid func","metadata":{}},{"cell_type":"code","source":"def mixup(input, truth, clip=[0, 1]):\n    indices = torch.randperm(input.size(0))\n    shuffled_input = input[indices]\n    shuffled_labels = truth[indices]\n\n    lam = np.random.uniform(clip[0], clip[1])\n    input = input * lam + shuffled_input * (1 - lam)\n    return input, truth, shuffled_labels, lam\n\n\ndef train_func(model, loader_train, optimizer, scaler=None):\n    model.train()\n    train_loss = []\n    bar = tqdm(loader_train)\n    for images, targets in bar:\n        optimizer.zero_grad()\n        images = images.cuda()\n        targets = targets.cuda()\n        \n        do_mixup = False\n        if random.random() < p_mixup:\n            do_mixup = True\n            images, targets, targets_mix, lam = mixup(images, targets)\n\n        with amp.autocast():\n            logits = model(images)\n            loss = criterion(logits, targets)\n            if do_mixup:\n                loss11 = criterion(logits, targets_mix)\n                loss = loss * lam  + loss11 * (1 - lam)\n        train_loss.append(loss.item())\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        bar.set_description(f'smth:{np.mean(train_loss[-30:]):.4f}')\n\n    return np.mean(train_loss)\n\n\ndef valid_func(model, loader_valid):\n    model.eval()\n    valid_loss = []\n    gts = []\n    outputs = []\n    bar = tqdm(loader_valid)\n    with torch.no_grad():\n        for images, targets in bar:\n            images = images.cuda()\n            targets = targets.cuda()\n\n            logits = model(images)\n            loss = criterion(logits, targets)\n            \n            gts.append(targets.cpu())\n            outputs.append(logits.cpu())\n            valid_loss.append(loss.item())\n            \n            bar.set_description(f'smth:{np.mean(valid_loss[-30:]):.4f}')\n\n    outputs = torch.cat(outputs)\n    gts = torch.cat(gts)\n    valid_loss = criterion(outputs, gts).item()\n\n    return valid_loss\n","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:32.751519Z","iopub.execute_input":"2022-10-29T08:46:32.75212Z","iopub.status.idle":"2022-10-29T08:46:32.766761Z","shell.execute_reply.started":"2022-10-29T08:46:32.752079Z","shell.execute_reply":"2022-10-29T08:46:32.765711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 20, 2\noptimizer = optim.AdamW(m.parameters(), lr=init_lr)\nscheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs, eta_min=eta_min)\n\nlrs = []\nfor epoch in range(1, n_epochs+1):\n    scheduler_cosine.step(epoch-1)\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nplt.plot(range(len(lrs)), lrs)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:32.770484Z","iopub.execute_input":"2022-10-29T08:46:32.770948Z","iopub.status.idle":"2022-10-29T08:46:33.001268Z","shell.execute_reply.started":"2022-10-29T08:46:32.77091Z","shell.execute_reply":"2022-10-29T08:46:33.000206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def run(fold):\n\n    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n\n    train_ = df[df['fold'] != fold].reset_index(drop=True)\n    valid_ = df[df['fold'] == fold].reset_index(drop=True)\n    dataset_train = CLSDataset(train_, 'train', transform=transforms_train)\n    dataset_valid = CLSDataset(valid_, 'valid', transform=transforms_valid)\n    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    model = TimmModel(backbone, pretrained=True)\n    model = model.to(device)\n\n    optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n\n    metric_best = np.inf\n    loss_min = np.inf\n\n    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, n_epochs, eta_min=eta_min)\n\n    print(len(dataset_train), len(dataset_valid))\n\n    for epoch in range(1, n_epochs+1):\n        scheduler_cosine.step(epoch-1)\n\n        print(time.ctime(), 'Epoch:', epoch)\n\n        train_loss = train_func(model, loader_train, optimizer, scaler)\n        valid_loss = valid_func(model, loader_valid)\n        metric = valid_loss\n\n        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f}.'\n        print(content)\n        with open(log_file, 'a') as appender:\n            appender.write(content + '\\n')\n\n        if metric < metric_best:\n            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n#             if not DEBUG:\n            torch.save(model.state_dict(), model_file)\n            metric_best = metric\n\n        # Save Last\n        if not DEBUG:\n            torch.save(\n                {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n                    'score_best': metric_best,\n                },\n                model_file.replace('_best', '_last')\n            )\n\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:33.003238Z","iopub.execute_input":"2022-10-29T08:46:33.003917Z","iopub.status.idle":"2022-10-29T08:46:33.01973Z","shell.execute_reply.started":"2022-10-29T08:46:33.003873Z","shell.execute_reply":"2022-10-29T08:46:33.018392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(0)\nrun(1)\nrun(2)\nrun(3)\nrun(4)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:33.021481Z","iopub.execute_input":"2022-10-29T08:46:33.021919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}