{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inference\n\nThis is a small version of inference, using: \n\n* 5-fold stage1 models (128x128x128)\n* 5-fold stage2 models (224x224)\n\n15 models in total.\n\nTo get higher metric score, you only need to:\n\n* use higher input resolution to train models.\n* use bigger backbones.\n* ensemble more models.","metadata":{}},{"cell_type":"code","source":"DEBUG = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nsys.path = [\n    '../input/covn3d-same',\n    '../input/timm20221011/pytorch-image-models-master',\n    '../input/smp20210127/segmentation_models.pytorch-master/segmentation_models.pytorch-master',\n    '../input/smp20210127/pretrained-models.pytorch-master/pretrained-models.pytorch-master',\n    '../input/smp20210127/EfficientNet-PyTorch-master/EfficientNet-PyTorch-master',\n] + sys.path\n\n!pip -q install ../input/pylibjpeg140py3/pylibjpeg-1.4.0-py3-none-any.whl\n!pip -q install ../input/pylibjpeg140py3/python_gdcm-3.0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n!cp -r ../input/timm-20220211/pytorch-image-models-master/timm ./timm4smp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-30T05:03:39.042811Z","iopub.execute_input":"2022-10-30T05:03:39.043303Z","iopub.status.idle":"2022-10-30T05:04:44.443348Z","shell.execute_reply.started":"2022-10-30T05:03:39.043204Z","shell.execute_reply":"2022-10-30T05:04:44.441665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport ast\nimport cv2\nimport time\nimport timm\nimport timm4smp\nimport pickle\nimport random\nimport pydicom\nimport argparse\nimport warnings\nimport threading\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom glob import glob\nimport albumentations\nimport matplotlib.pyplot as plt\nimport segmentation_models_pytorch as smp\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.cuda.amp as amp\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom pylab import rcParams\n\n%matplotlib inline\ndevice = torch.device('cuda')\ntorch.backends.cudnn.benchmark = True\n\ntimm.__version__, timm4smp.__version__","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:36.366529Z","iopub.execute_input":"2022-10-30T05:05:36.3673Z","iopub.status.idle":"2022-10-30T05:05:36.383599Z","shell.execute_reply.started":"2022-10-30T05:05:36.367257Z","shell.execute_reply":"2022-10-30T05:05:36.38257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/rsna-2022-cervical-spine-fracture-detection/'\nimage_size_seg = (128, 128, 128)\nmsk_size = image_size_seg[0]\nimage_size_cls = 224\nn_slice_per_c = 15\nn_ch = 5\n\nbatch_size_seg = 1\nnum_workers = 2","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:38.243965Z","iopub.execute_input":"2022-10-30T05:05:38.24436Z","iopub.status.idle":"2022-10-30T05:05:38.251074Z","shell.execute_reply.started":"2022-10-30T05:05:38.244329Z","shell.execute_reply":"2022-10-30T05:05:38.250013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    df = pd.read_csv(os.path.join(data_dir, 'train.csv')).head(1500)\n    df = pd.DataFrame({\n        'StudyInstanceUID': df['StudyInstanceUID'].unique().tolist()\n    })\n    df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\nelse:\n    df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n    if df.iloc[0].row_id == '1.2.826.0.1.3680043.10197_C1':\n        # test_images and test.csv are inconsistent in the dev dataset, fixing labels for the dev run.\n        df = pd.DataFrame({\n            \"row_id\": ['1.2.826.0.1.3680043.22327_C1', '1.2.826.0.1.3680043.25399_C1', '1.2.826.0.1.3680043.5876_C1'],\n            \"StudyInstanceUID\": ['1.2.826.0.1.3680043.22327', '1.2.826.0.1.3680043.25399', '1.2.826.0.1.3680043.5876'],\n            \"prediction_type\": [\"C1\", \"C1\", \"patient_overall\"]}\n        )\n    df = pd.DataFrame({\n        'StudyInstanceUID': df['StudyInstanceUID'].unique().tolist()\n    })\n    df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'test_images', x))\n\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:38.550234Z","iopub.execute_input":"2022-10-30T05:05:38.550565Z","iopub.status.idle":"2022-10-30T05:05:38.593693Z","shell.execute_reply.started":"2022-10-30T05:05:38.550535Z","shell.execute_reply":"2022-10-30T05:05:38.592753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"def load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    data = cv2.resize(data, (image_size_seg[0], image_size_seg[1]), interpolation = cv2.INTER_AREA)\n    return data\n\n\ndef load_dicom_line_par(path):\n\n    t_paths = sorted(glob(os.path.join(path, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n\n    n_scans = len(t_paths)\n#     print(n_scans)\n    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_size_seg[2])).round().astype(int)\n    t_paths = [t_paths[i] for i in indices]\n\n    images = []\n    for filename in t_paths:\n        images.append(load_dicom(filename))\n    images = np.stack(images, -1)\n    \n    images = images - np.min(images)\n    images = images / (np.max(images) + 1e-4)\n    images = (images * 255).astype(np.uint8)\n\n    return images\n\n\nclass SegTestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df = df.reset_index()\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n\n        image = load_dicom_line_par(row.image_folder)\n        if image.ndim < 4:\n            image = np.expand_dims(image, 0)\n        image = image.astype(np.float32).repeat(3, 0)  # to 3ch\n        image = image / 255.\n        return torch.tensor(image).float()\n","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:39.720643Z","iopub.execute_input":"2022-10-30T05:05:39.722018Z","iopub.status.idle":"2022-10-30T05:05:39.733841Z","shell.execute_reply.started":"2022-10-30T05:05:39.721971Z","shell.execute_reply":"2022-10-30T05:05:39.732824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataset_seg = SegTestDataset(df)\nloader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:39.883882Z","iopub.execute_input":"2022-10-30T05:05:39.884168Z","iopub.status.idle":"2022-10-30T05:05:39.890695Z","shell.execute_reply.started":"2022-10-30T05:05:39.884141Z","shell.execute_reply":"2022-10-30T05:05:39.889521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    rcParams['figure.figsize'] = 20,8\n    for i in range(2):\n        f, axarr = plt.subplots(1,4)\n        for p in range(4):\n            idx = i*4+p\n            img = dataset_seg[idx]\n            img = img[:, :, :, 60]\n            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:41.258937Z","iopub.execute_input":"2022-10-30T05:05:41.259986Z","iopub.status.idle":"2022-10-30T05:05:41.267087Z","shell.execute_reply.started":"2022-10-30T05:05:41.25994Z","shell.execute_reply":"2022-10-30T05:05:41.265796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    rcParams['figure.figsize'] = 20,8\n    for i in range(2):\n        f, axarr = plt.subplots(1,4)\n        for p in range(4):\n            idx = i*4+p\n            img = dataset_seg[idx]\n            img = img[:, :, 60, :]\n            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:41.48024Z","iopub.execute_input":"2022-10-30T05:05:41.480865Z","iopub.status.idle":"2022-10-30T05:05:41.488224Z","shell.execute_reply.started":"2022-10-30T05:05:41.480823Z","shell.execute_reply":"2022-10-30T05:05:41.486893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    rcParams['figure.figsize'] = 20,8\n    for i in range(2):\n        f, axarr = plt.subplots(1,4)\n        for p in range(4):\n            idx = i*4+p\n            img = dataset_seg[idx]\n            img = img[:, 60, :, :]\n            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:41.650555Z","iopub.execute_input":"2022-10-30T05:05:41.650857Z","iopub.status.idle":"2022-10-30T05:05:41.657196Z","shell.execute_reply.started":"2022-10-30T05:05:41.650829Z","shell.execute_reply":"2022-10-30T05:05:41.655984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from timm4smp.models.layers.conv2d_same import Conv2dSame\nfrom conv3d_same import Conv3dSame\n\ndef convert_3d(module):\n\n    module_output = module\n    if isinstance(module, torch.nn.BatchNorm2d):\n        module_output = torch.nn.BatchNorm3d(\n            module.num_features,\n            module.eps,\n            module.momentum,\n            module.affine,\n            module.track_running_stats,\n        )\n        if module.affine:\n            with torch.no_grad():\n                module_output.weight = module.weight\n                module_output.bias = module.bias\n        module_output.running_mean = module.running_mean\n        module_output.running_var = module.running_var\n        module_output.num_batches_tracked = module.num_batches_tracked\n        if hasattr(module, \"qconfig\"):\n            module_output.qconfig = module.qconfig\n            \n    elif isinstance(module, Conv2dSame):\n        module_output = Conv3dSame(\n            in_channels=module.in_channels,\n            out_channels=module.out_channels,\n            kernel_size=module.kernel_size[0],\n            stride=module.stride[0],\n            padding=module.padding[0],\n            dilation=module.dilation[0],\n            groups=module.groups,\n            bias=module.bias is not None,\n        )\n        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n\n    elif isinstance(module, torch.nn.Conv2d):\n        module_output = torch.nn.Conv3d(\n            in_channels=module.in_channels,\n            out_channels=module.out_channels,\n            kernel_size=module.kernel_size[0],\n            stride=module.stride[0],\n            padding=module.padding[0],\n            dilation=module.dilation[0],\n            groups=module.groups,\n            bias=module.bias is not None,\n            padding_mode=module.padding_mode\n        )\n        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n\n    elif isinstance(module, torch.nn.MaxPool2d):\n        module_output = torch.nn.MaxPool3d(\n            kernel_size=module.kernel_size,\n            stride=module.stride,\n            padding=module.padding,\n            dilation=module.dilation,\n            ceil_mode=module.ceil_mode,\n        )\n    elif isinstance(module, torch.nn.AvgPool2d):\n        module_output = torch.nn.AvgPool3d(\n            kernel_size=module.kernel_size,\n            stride=module.stride,\n            padding=module.padding,\n            ceil_mode=module.ceil_mode,\n        )\n\n    for name, child in module.named_children():\n        module_output.add_module(\n            name, convert_3d(child)\n        )\n    del module\n\n    return module_output\n\n\n\nclass TimmSegModel(nn.Module):\n    def __init__(self, backbone, segtype='unet', pretrained=False):\n        super(TimmSegModel, self).__init__()\n\n        self.encoder = timm4smp.create_model(\n            backbone,\n            in_chans=3,\n            features_only=True,\n            pretrained=pretrained\n        )\n        g = self.encoder(torch.rand(1, 3, 64, 64))\n        encoder_channels = [1] + [_.shape[1] for _ in g]\n        decoder_channels = [256, 128, 64, 32, 16]\n        if segtype == 'unet':\n            self.decoder = smp.unet.decoder.UnetDecoder(\n                encoder_channels=encoder_channels[:n_blocks+1],\n                decoder_channels=decoder_channels[:n_blocks],\n                n_blocks=n_blocks,\n            )\n        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n    def forward(self,x):\n        global_features = [0] + self.encoder(x)[:n_blocks]\n        seg_features = self.decoder(*global_features)\n        seg_features = self.segmentation_head(seg_features)\n        return seg_features\n    \n    \nclass TimmModel(nn.Module):\n    def __init__(self, backbone, image_size, pretrained=False):\n        super(TimmModel, self).__init__()\n        self.image_size = image_size\n        self.encoder = timm.create_model(\n            backbone,\n            in_chans=in_chans,\n            num_classes=1,\n            features_only=False,\n            drop_rate=0,\n            drop_path_rate=0,\n            pretrained=pretrained\n        )\n\n        if 'efficient' in backbone:\n            hdim = self.encoder.conv_head.out_channels\n            self.encoder.classifier = nn.Identity()\n        elif 'convnext' in backbone or 'nfnet' in backbone:\n            hdim = self.encoder.head.fc.in_features\n            self.encoder.head.fc = nn.Identity()\n\n        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n        self.head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, 1),\n        )\n        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n        self.head2 = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, 1),\n        )\n\n\n    def forward(self, x):  # (bs, nc*7, ch, sz, sz)\n        bs = x.shape[0]\n        x = x.view(bs * n_slice_per_c * 7, in_chans, self.image_size, self.image_size)\n        feat = self.encoder(x)\n        feat = feat.view(bs, n_slice_per_c * 7, -1)\n        feat1, _ = self.lstm(feat)\n        feat1 = feat1.contiguous().view(bs * n_slice_per_c * 7, 512)\n        feat2, _ = self.lstm2(feat)\n\n        return self.head(feat1), self.head2(feat2[:, 0])\n    \n    \n    \n    \nclass Timm1BoneModel(nn.Module):\n    def __init__(self, backbone, image_size, pretrained=False):\n        super(Timm1BoneModel, self).__init__()\n        self.image_size = image_size\n\n        self.encoder = timm.create_model(\n            backbone,\n            in_chans=in_chans,\n            num_classes=1,\n            features_only=False,\n            drop_rate=0,\n            drop_path_rate=0,\n            pretrained=pretrained\n        )\n\n        if 'efficient' in backbone:\n            hdim = self.encoder.conv_head.out_channels\n            self.encoder.classifier = nn.Identity()\n        elif 'convnext' in backbone or 'nfnet' in backbone:\n            hdim = self.encoder.head.fc.in_features\n            self.encoder.head.fc = nn.Identity()\n\n        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n        self.head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, 1),\n        )\n\n\n    def forward(self, x):  # (bs, nslice, ch, sz, sz)\n        bs = x.shape[0]\n        x = x.view(bs * n_slice_per_c, in_chans, self.image_size, self.image_size)\n        feat = self.encoder(x)\n        feat = feat.view(bs, n_slice_per_c, -1)\n        feat, _ = self.lstm(feat)\n        feat = feat.contiguous().view(bs * n_slice_per_c, -1)\n        feat = self.head(feat)\n        feat = feat.view(bs, n_slice_per_c).contiguous()\n\n        return feat\n","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:05:58.414309Z","iopub.execute_input":"2022-10-30T05:05:58.414747Z","iopub.status.idle":"2022-10-30T05:05:58.459664Z","shell.execute_reply.started":"2022-10-30T05:05:58.414692Z","shell.execute_reply":"2022-10-30T05:05:58.458503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Models","metadata":{}},{"cell_type":"code","source":"models_seg = []\n\nkernel_type = 'timm3d_v2s_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_mixup1_lr1e3_20x50ep'\nbackbone = 'tf_efficientnetv2_s_in21ft1k'\nmodel_dir_seg = '../input/seg-v2s-0911/'\nn_blocks = 4\nfor fold in range(5):\n    model = TimmSegModel(backbone, pretrained=False)\n    model = convert_3d(model)\n    model = model.to(device)\n    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n    sd = torch.load(load_model_file)\n    if 'model_state_dict' in sd.keys():\n        sd = sd['model_state_dict']\n    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n    model.load_state_dict(sd, strict=True)\n    model.eval()\n    models_seg.append(model)\n\nlen(models_seg)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:06:02.247561Z","iopub.execute_input":"2022-10-30T05:06:02.2482Z","iopub.status.idle":"2022-10-30T05:06:18.464686Z","shell.execute_reply.started":"2022-10-30T05:06:02.248158Z","shell.execute_reply":"2022-10-30T05:06:18.46365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_type = '0920_1bonev2_effv2s_224_15_6ch_augv2_mixupp5_drl3_rov1p2_bs8_lr23e6_eta23e6_75ep'\nmodel_dir_cls = '../input/rsna-stage2-type1-v2s-224/'\nbackbone = 'tf_efficientnetv2_s_in21ft1k'\nin_chans = 6\nmodels_cls1 = []\n\nfor fold in range(5):\n    model = Timm1BoneModel(backbone, image_size=224, pretrained=False)\n    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n    sd = torch.load(load_model_file, map_location='cpu')\n    if 'model_state_dict' in sd.keys():\n        sd = sd['model_state_dict']\n    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n    model.load_state_dict(sd, strict=True)\n    model = model.to(device)\n    model.eval()\n    models_cls1.append(model)\n\nlen(models_cls1)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:11:05.27204Z","iopub.execute_input":"2022-10-30T05:11:05.272444Z","iopub.status.idle":"2022-10-30T05:11:09.237891Z","shell.execute_reply.started":"2022-10-30T05:11:05.272406Z","shell.execute_reply":"2022-10-30T05:11:09.236646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_type = '0920_2d_lstmv22headv2_convnn_224_15_6ch_8flip_augv2_drl3_rov1p2_rov3p2_bs4_lr6e6_eta6e6_lw151_50ep'\nmodel_dir_cls = '../input/rsna-stage2-type2-convnn-224'\nbackbone = 'convnext_nano'\nin_chans = 6\nmodels_cls2 = []\n\nfor fold in range(5):\n    model = TimmModel(backbone, image_size=224, pretrained=False)\n    model = model.to(device)\n    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n    sd = torch.load(load_model_file)\n    if 'model_state_dict' in sd.keys():\n        sd = sd['model_state_dict']\n    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n    model.load_state_dict(sd, strict=True)\n    model.eval()\n    models_cls2.append(model)\n\nlen(models_cls2)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:11:09.240407Z","iopub.execute_input":"2022-10-30T05:11:09.240885Z","iopub.status.idle":"2022-10-30T05:11:11.795864Z","shell.execute_reply.started":"2022-10-30T05:11:09.240846Z","shell.execute_reply":"2022-10-30T05:11:11.794641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:11:11.797505Z","iopub.execute_input":"2022-10-30T05:11:11.798532Z","iopub.status.idle":"2022-10-30T05:11:12.902623Z","shell.execute_reply.started":"2022-10-30T05:11:11.798485Z","shell.execute_reply":"2022-10-30T05:11:12.901322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_bone(msk, cid, t_paths, cropped_images):\n    n_scans = len(t_paths)\n    bone = []\n    try:\n        msk_b = msk[cid] > 0.2\n        msk_c = msk[cid] > 0.05\n\n        x = np.where(msk_b.sum(1).sum(1) > 0)[0]\n        y = np.where(msk_b.sum(0).sum(1) > 0)[0]\n        z = np.where(msk_b.sum(0).sum(0) > 0)[0]\n\n        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n            x = np.where(msk_c.sum(1).sum(1) > 0)[0]\n            y = np.where(msk_c.sum(0).sum(1) > 0)[0]\n            z = np.where(msk_c.sum(0).sum(0) > 0)[0]\n\n        x1, x2 = max(0, x[0] - 1), min(msk.shape[1], x[-1] + 1)\n        y1, y2 = max(0, y[0] - 1), min(msk.shape[2], y[-1] + 1)\n        z1, z2 = max(0, z[0] - 1), min(msk.shape[3], z[-1] + 1)\n        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans)\n\n        inds = np.linspace(zz1 ,zz2-1 ,n_slice_per_c).astype(int)\n        inds_ = np.linspace(z1 ,z2-1 ,n_slice_per_c).astype(int)\n        for sid, (ind, ind_) in enumerate(zip(inds, inds_)):\n\n            msk_this = msk[cid, :, :, ind_]\n\n            images = []\n            for i in range(-n_ch//2+1, n_ch//2+1):\n                try:\n                    dicom = pydicom.read_file(t_paths[ind+i])\n                    images.append(dicom.pixel_array)\n                except:\n                    images.append(np.zeros((512, 512)))\n\n            data = np.stack(images, -1)\n            data = data - np.min(data)\n            data = data / (np.max(data) + 1e-4)\n            data = (data * 255).astype(np.uint8)\n            msk_this = msk_this[x1:x2, y1:y2]\n            xx1 = int(x1 / msk_size * data.shape[0])\n            xx2 = int(x2 / msk_size * data.shape[0])\n            yy1 = int(y1 / msk_size * data.shape[1])\n            yy2 = int(y2 / msk_size * data.shape[1])\n            data = data[xx1:xx2, yy1:yy2]\n            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n            msk_this = (msk_this * 255).astype(np.uint8)\n            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)\n\n            data = np.concatenate([data, msk_this[:, :, np.newaxis]], -1)\n\n            bone.append(torch.tensor(data))\n\n    except:\n        for sid in range(n_slice_per_c):\n            bone.append(torch.ones((image_size_cls, image_size_cls, n_ch+1)).int())\n\n    cropped_images[cid] = torch.stack(bone, 0)\n\n\ndef load_cropped_images(msk, image_folder, n_ch=n_ch):\n\n    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n    for cid in range(7):\n        threads[cid] = threading.Thread(target=load_bone, args=(msk, cid, t_paths, cropped_images))\n        threads[cid].start()\n    for cid in range(7):\n        threads[cid].join()\n\n    return torch.cat(cropped_images, 0)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:11:12.906193Z","iopub.execute_input":"2022-10-30T05:11:12.906693Z","iopub.status.idle":"2022-10-30T05:11:12.928799Z","shell.execute_reply.started":"2022-10-30T05:11:12.906652Z","shell.execute_reply":"2022-10-30T05:11:12.927486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"outputs1 = []\noutputs2 = []\n\nbar = tqdm(loader_seg)\nwith torch.no_grad():\n    for batch_id, (images) in enumerate(bar):\n        images = images.cuda()\n\n        # SEG\n        pred_masks = []\n        for model in models_seg:\n            pmask = model(images).sigmoid()\n            pred_masks.append(pmask)\n        pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()\n\n        # Build cls input\n        cls_inp = []\n        threads = [None] * 7\n        cropped_images = [None] * 7\n\n        for i in range(pred_masks.shape[0]):\n            row = df.iloc[batch_id*batch_size_seg+i]\n            cropped_images = load_cropped_images(pred_masks[i], row.image_folder)\n            cls_inp.append(cropped_images.permute(0, 3, 1, 2).float() / 255.)\n        cls_inp = torch.stack(cls_inp, 0).to(device)  # (1, 105, 6, 224, 224)\n\n        pred_cls1, pred_cls2 = [], []\n        # CLS 2\n        for _, model in enumerate(models_cls2):\n            logits, logits2 = model(cls_inp)\n            pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n            pred_cls2.append(logits2.sigmoid())\n\n        # CLS 1\n        cls_inp = cls_inp.view(7, 15, 6, image_size_cls, image_size_cls).contiguous()\n        for _, model in enumerate(models_cls1):\n            logits = model(cls_inp)\n            pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n\n        pred_cls1 = torch.stack(pred_cls1, 0).mean(0)\n        pred_cls2 = torch.stack(pred_cls2, 0).mean(0)\n        outputs1.append(pred_cls1.cpu())\n        outputs2.append(pred_cls2.cpu())\n","metadata":{"execution":{"iopub.status.busy":"2022-10-30T05:11:35.770051Z","iopub.execute_input":"2022-10-30T05:11:35.770395Z","iopub.status.idle":"2022-10-30T05:11:43.226734Z","shell.execute_reply.started":"2022-10-30T05:11:35.770366Z","shell.execute_reply":"2022-10-30T05:11:43.225225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Output","metadata":{}},{"cell_type":"code","source":"outputs1 = torch.cat(outputs1)\noutputs2 = torch.cat(outputs2)","metadata":{"execution":{"iopub.status.busy":"2022-10-21T11:38:43.095123Z","iopub.execute_input":"2022-10-21T11:38:43.096666Z","iopub.status.idle":"2022-10-21T11:38:43.119061Z","shell.execute_reply.started":"2022-10-21T11:38:43.096623Z","shell.execute_reply":"2022-10-21T11:38:43.118069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRED1 = (outputs1.mean(-1)).clamp(0.0001, 0.9999)\nPRED2 = (outputs2.view(-1)).clamp(0.0001, 0.9999)","metadata":{"execution":{"iopub.status.busy":"2022-10-21T11:38:43.120761Z","iopub.execute_input":"2022-10-21T11:38:43.121396Z","iopub.status.idle":"2022-10-21T11:38:43.135306Z","shell.execute_reply.started":"2022-10-21T11:38:43.121355Z","shell.execute_reply":"2022-10-21T11:38:43.134234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_ids = []\nfor _, row in df.iterrows():\n    for i in range(7):\n        row_ids.append(row.StudyInstanceUID + f'_C{i+1}')\n    row_ids.append(row.StudyInstanceUID + '_patient_overall')","metadata":{"execution":{"iopub.status.busy":"2022-10-21T11:38:43.136776Z","iopub.execute_input":"2022-10-21T11:38:43.137361Z","iopub.status.idle":"2022-10-21T11:38:43.14516Z","shell.execute_reply.started":"2022-10-21T11:38:43.137324Z","shell.execute_reply":"2022-10-21T11:38:43.144134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = pd.DataFrame({\n    'row_id': row_ids,\n    'fractured': torch.cat([PRED1, PRED2.unsqueeze(1)], 1).view(-1),\n})","metadata":{"execution":{"iopub.status.busy":"2022-10-21T11:38:43.146675Z","iopub.execute_input":"2022-10-21T11:38:43.147449Z","iopub.status.idle":"2022-10-21T11:38:43.154873Z","shell.execute_reply.started":"2022-10-21T11:38:43.147397Z","shell.execute_reply":"2022-10-21T11:38:43.153934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-21T11:38:43.156122Z","iopub.execute_input":"2022-10-21T11:38:43.15722Z","iopub.status.idle":"2022-10-21T11:38:43.167857Z","shell.execute_reply.started":"2022-10-21T11:38:43.157186Z","shell.execute_reply":"2022-10-21T11:38:43.166934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub","metadata":{"execution":{"iopub.status.busy":"2022-10-21T11:38:43.169507Z","iopub.execute_input":"2022-10-21T11:38:43.17Z","iopub.status.idle":"2022-10-21T11:38:43.182118Z","shell.execute_reply.started":"2022-10-21T11:38:43.169962Z","shell.execute_reply":"2022-10-21T11:38:43.181084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}